<!DOCTYPE html>
<html lang="en-us">

<head>
  <title>Building a data pipeline for displaying Kubernetes public artifact traffic | ii.nz</title>

  <meta name="ii" content="cool">
  <meta charset="UTF-8">
  <meta name="language" content="en">
  <meta name="description" content="">
  <meta name="keywords" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  
  

  <link rel="shortcut icon" type="image/png" href="/favicon.ico" />


  
  
    
 
  
  

  

  
    
    <link type="text/css" rel="stylesheet" href="/css/post.min.2cdedeaee06d510cb6af67a8b95ad416aaefd78e2001e4f3e84bd07b2aa0e45d.css" integrity="sha256-LN7eruBtUQy2r2eouVrUFqrv144gAeTz6EvQeyqg5F0="/>
  
    
    <link type="text/css" rel="stylesheet" href="/css/custom.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css" integrity="sha256-47DEQpj8HBSa&#43;/TImW&#43;5JCeuQeRkm5NMpJWZG3hSuFU="/>
  

  
   
  

<script type="application/ld+json">
  
    {
      "@context": "http://schema.org",
      "@type": "WebSite",
      "url": "https:\/\/ii.nz\/post\/building-a-data-pipeline-for-displaying-kubernetes-public-artifact-traffic\/",
      "name": "Building a data pipeline for displaying Kubernetes public artifact traffic",
      "author": {
        "@type": "Person",
        "name": ""
      },
      "description": ""
    }
  
  </script>
<link rel='stylesheet' type='text/css' href='/ii-style.css'>
</head>

<body>
  <div class="burger__container">
  <div class="burger" aria-controls="navigation" aria-label="Menu">
    <div class="burger__meat burger__meat--1"></div>
    <div class="burger__meat burger__meat--2"></div>
    <div class="burger__meat burger__meat--3"></div>
  </div>
    <h1>ii.nz<span class="fancy">.</span></h1>
</div>


  <nav class="nav" id="navigation">
  <ul class="nav__list">
    
    
      <li>
        <a  href="/">home üè†</a>
      </li>
    
      <li>
        <a  href="/about">about üßÑ</a>
      </li>
    
      <li>
        <a  href="/post">posts üìú</a>
      </li>
    
      <li>
        <a  href="/stories">stories üó®Ô∏è </a>
      </li>
    
  </ul>
</nav>


  <main>
    
    

    <div class="flex-wrapper">
      <div class="post__container">
        <div class="post">
          <header class="post__header">
            
            <div class="post__metadata">
              <h1 id="post__title">Building a data pipeline for displaying Kubernetes public artifact traffic</h1>
              üìÜ<time datetime="2021-08-24 00:00:00 &#43;0000 UTC" class="post__date">Aug 24 2021</time> 
              <span class="post__date">üïö 7 min read</span>
              <div class="authors">
                by
                
                <a class="author" href=/author/caleb-woodbine>
                  Caleb Woodbine
                </a>
                
              </div>
              
            </div>
          </header>
          <article class="post__content">
            
<h2 id="introduction"><a href="#introduction" class="anchor">#</a>Introduction</h2>
<p>ii is a member of the <a href="https://github.com/kubernetes/community/blob/master/wg-k8s-infra/README.md" 
  
   target="_blank" rel="noreferrer noopener" 
>Kubernetes Infra working group</a>
, the group responsible for defining and managing the infrastructure for the Kubernetes project.
The infrastructure includes but is not limited to:</p>
<ul>
<li><a href="https://prow.k8s.io" 
  
   target="_blank" rel="noreferrer noopener" 
>prow.k8s.io</a>
</li>
<li><a href="https://github.com/kubernetes/k8s.io/tree/main/apps/slack-infra" 
  
   target="_blank" rel="noreferrer noopener" 
>Slack infra</a>
</li>
<li><a href="https://github.com/kubernetes/k8s.io/tree/main/dns" 
  
   target="_blank" rel="noreferrer noopener" 
>DNS records</a>
</li>
<li><a href="https://github.com/kubernetes/k8s.io/blob/main/apps/k8s-io/README.md" 
  
   target="_blank" rel="noreferrer noopener" 
>k8s.io redirects</a>
</li>
<li>project and resource management for SIGs and WGs</li>
</ul>
<p>One of goals of the group is to discover where the costs are coming from and encorage large traffic users to self-host / cache the artifacts on their side, as well as to spend the funds better for the entirety of the Kubernetes infrastructure.
In order to do this, we first need to discover where the traffic is coming from.</p>
<p>With access to a bucket containing the logs of traffic over a certain time period, we&rsquo;re able to parse it and calculate a few things.</p>
<p>Organisations publish and advertise their IP addresses through <a href="https://en.wikipedia.org/wiki/Border%5FGateway%5FProtocol" 
  
   target="_blank" rel="noreferrer noopener" 
>BGP</a>
, a fundamental sub-sytem of the internet.
IP addresses are published in blocks (subnetted) in <a href="https://en.wikipedia.org/wiki/Autonomous%5Fsystem%5F%28Internet%29" 
  
   target="_blank" rel="noreferrer noopener" 
>ASN</a>
 data.
With that in mind, the ASN data of cloud-providers and organisations that fund the CNCF for Kubernetes are publically available, through this we&rsquo;re able to figure out the Kubernetes Infra project should communicate to about their traffic usage.</p>
<h2 id="considering-steps"><a href="#considering-steps" class="anchor">#</a>Considering steps</h2>
<p>At the beginning, the pieces of this puzzle were less known so it was considered to be something like this</p>
<img style='margin-left: auto; margin-right: auto;' alt='asn-data-pipeline-plan' src='/images/2021/asn-data-pipeline-plan.svg'>
<h2 id="planning-out-the-pipeline"><a href="#planning-out-the-pipeline" class="anchor">#</a>Planning out the pipeline</h2>
<p>After some more research and discovery, here is the pipeline architecture</p>
<img style='margin-left: auto; margin-right: auto;' alt='asn-data-pipeline' src='/images/2021/asn-data-pipeline.svg'>
<p>Our data sources are:</p>
<ul>
<li>global ASN data (<a href="https://bgp.potaroo.net/" 
  
   target="_blank" rel="noreferrer noopener" 
>Potaroo</a>
, <a href="https://github.com/hadiasghari/pyasn" 
  
   target="_blank" rel="noreferrer noopener" 
>PyASN</a>
)</li>
<li>ASN-to-vendor metadata YAML</li>
<li>Kubernetes public artifact logs</li>
</ul>
<p>ii is a heavy user of Postgres for data transformation and processing (see <a href="https://github.com/cncf/apisnoop/tree/main/apps/snoopdb" 
  
   target="_blank" rel="noreferrer noopener" 
>APISnoop and SnoopDB</a>
).
Using these sources, we&rsquo;ll do processing in a Postgres container and in BigQuery.
We&rsquo;re using BigQuery, since we&rsquo;ll need to display the data in DataStudio for our <a href="https://github.com/kubernetes/community/blob/master/wg-k8s-infra/README.md#meetings" 
  
   target="_blank" rel="noreferrer noopener" 
>bi-weekly Thursday (NZT) wg-k8s-infra meetings</a>
.</p>
<p>The data will need to be reproducible via the job running once again.</p>
<h2 id="finding-asn-databases"><a href="#finding-asn-databases" class="anchor">#</a>Finding ASN databases</h2>
<p>ASN data is important for this operation, because it allows us to find who owns the IP addresses which are making requests to the artifact servers via the logs.
All ASN data is public, which makes it easy to discover these IP blocks from ASNs and match them to their known owner.</p>
<p>In the pipeline, PyASN is used with some custom logic to parse ASN data provided by the Potaroo ASN database (which could be any provider).</p>
<h2 id="the-pipeline-of-the-asn-and-ip-to-vendor-discussion"><a href="#the-pipeline-of-the-asn-and-ip-to-vendor-discussion" class="anchor">#</a>The pipeline of the ASN and IP to vendor discussion</h2>
<img style='margin-left: auto; margin-right: auto;' alt='asn-ip-pipeline' src='/images/2021/asn-ip-pipeline.svg'>
<h2 id="gathering-the-asns-of-cncf-supporting-organisations"><a href="#gathering-the-asns-of-cncf-supporting-organisations" class="anchor">#</a>Gathering the ASNs of CNCF supporting organisations</h2>
<p>There are quite a number of organisations that support the CNCF and consume Kubernetes artifacts.
After considering a handful of organsations to begin with, the ASNs are discovered through sites like <a href="https://www.peeringdb.com/" 
  
   target="_blank" rel="noreferrer noopener" 
>PeeringDB</a>
 and committing into some files in this repo
<a href="https://github.com/kubernetes/k8s.io/tree/main/registry.k8s.io/infra/meta/asns" 
  
   target="_blank" rel="noreferrer noopener" 
>k8s.io/registry.k8s.io/infra/meta/asns</a>

for later parsing and reading.</p>
<p>The ASNs are reviewed and verified by a member of the organisation that ii has a relationship with or someone through the relationship. Some organisations may not wish to verify this collected public data, in that case we will just trust it.</p>
<p>The metadata file will also contain directions on a later service to possibly handle the redirections to the closest cloud-provider, based on the request.</p>
<h2 id="kubernetes-public-artifact-logs"><a href="#kubernetes-public-artifact-logs" class="anchor">#</a>Kubernetes Public Artifact Logs</h2>
<p>A GCP project in the Kubernetes org was provisioned to house the PII (Publicly Identifing Information) logs of the <a href="https://cloud.google.com/container-registry/" 
  
   target="_blank" rel="noreferrer noopener" 
>GCR</a>
 logs and artifacts logs in a <a href="https://cloud.google.com/storage" 
  
   target="_blank" rel="noreferrer noopener" 
>GCS</a>
 bucket.</p>
<h2 id="postgres-processing"><a href="#postgres-processing" class="anchor">#</a>Postgres processing</h2>
<p>The largest part of the data transformation happens in a Postgres Pod, running as a Prow Job.
Firstly, we bring up Postgres and begin to run the pg-init scripts.</p>
<p>A dataset is created in the <em>kubernetes-public-ii-sandbox</em> project, Potaroo pre-processed data is downloaded along with PyASN data, and PyASN data is converted for use.</p>
<pre><code class="language-shell">bq mk \
    --dataset \
    --description &quot;etl pipeline dataset for ASN data from CNCF supporting vendors of k8s infrastructure&quot; \
    &quot;${GCP_PROJECT}:${GCP_BIGQUERY_DATASET}_$(date +%Y%m%d)&quot;
# ...
gsutil cp gs://ii_bq_scratch_dump/potaroo_company_asn.csv  /tmp/potaroo_data.csv
# ...
pyasn_util_download.py --latest
# ...
python3 /app/ip-from-pyasn.py /tmp/potaroo_asn.txt ipasn_latest.dat /tmp/pyAsnOutput.csv
</code></pre>
<p>(see <a href="https://github.com/ii/org/blob/main/research/asn-data-pipeline/asn%5Fpipeline%5Fdocker%5Ffile.org#get-dependencies" 
  
   target="_blank" rel="noreferrer noopener" 
>stage get dependencies</a>
)</p>
<p>Tables are created so that the data can be stored and processed.
(see <a href="https://github.com/ii/org/blob/main/research/asn-data-pipeline/asn%5Fpipeline%5Fdocker%5Ffile.org#migrate-schemas" 
  
   target="_blank" rel="noreferrer noopener" 
>migrate schemas</a>
)</p>
<p>Data is now loaded from the local ASN data and outputted as CSV for use shortly.</p>
<pre><code class="language-sql-mode">copy (select * from pyasn_ip_asn_extended) to '/tmp/pyasn_expanded_ipv4.csv' csv header;
</code></pre>
<p>(see <a href="https://github.com/ii/org/blob/main/research/asn-data-pipeline/asn%5Fpipeline%5Fdocker%5Ffile.org#load-pyasn-data-into-postgres" 
  
   target="_blank" rel="noreferrer noopener" 
>stage load PyASN data into Postgres</a>
)</p>
<p>The data is now uploaded to ASN BigQuery for later use.</p>
<p>Next, the vendor ASN metadata is downloaded from GitHub</p>
<pre><code class="language-shell">for VENDOR in ${VENDORS[*]}; do
  curl -s &quot;https://raw.githubusercontent.com/kubernetes/k8s.io/main/registry.k8s.io/infra/meta/asns/${VENDOR}.yaml&quot; \
      | yq e . -j - \
      | jq -r '.name as $name | .redirectsTo.registry as $redirectsToRegistry | .redirectsTo.artifacts as $redirectsToArtifacts | .asns[] | [. ,$name, $redirectsToRegistry, $redirectsToArtifacts] | @csv' \
        &gt; &quot;/tmp/vendor/${VENDOR}_yaml.csv&quot;
  bq load --autodetect &quot;${GCP_BIGQUERY_DATASET}_$(date +%Y%m%d).vendor_yaml&quot; &quot;/tmp/vendor/${VENDOR}_yaml.csv&quot; asn_yaml:integer,name_yaml:string,redirectsToRegistry:string,redirectsToArtifacts:string
done
</code></pre>
<p>along with the IP blocks from major cloud-providers and uploaded to BigQuery</p>
<pre><code class="language-shell">curl &quot;https://download.microsoft.com/download/7/1/D/71D86715-5596-4529-9B13-DA13A5DE5B63/ServiceTags_Public_20210802.json&quot; \
    | jq -r '.values[] | .properties.platform as $service | .properties.region as $region | .properties.addressPrefixes[] | [., $service, $region] | @csv' \
      &gt; /tmp/vendor/microsoft_raw_subnet_region.csv
curl 'https://www.gstatic.com/ipranges/cloud.json' \
    | jq -r '.prefixes[] | [.ipv4Prefix, .service, .scope] | @csv' \
      &gt; /tmp/vendor/google_raw_subnet_region.csv
curl 'https://ip-ranges.amazonaws.com/ip-ranges.json' \
    | jq -r '.prefixes[] | [.ip_prefix, .service, .region] | @csv' \
      &gt; /tmp/vendor/amazon_raw_subnet_region.csv
</code></pre>
<p>and the PeeringDB tables are downloaded via the API</p>
<pre><code class="language-shell">mkdir -p /tmp/peeringdb-tables
PEERINGDB_TABLES=(
    net
    poc
)
for PEERINGDB_TABLE in ${PEERINGDB_TABLES[*]}; do
    curl -sG &quot;https://www.peeringdb.com/api/${PEERINGDB_TABLE}&quot; | jq -c '.data[]' | sed 's,&quot;,\&quot;,g' &gt; &quot;/tmp/peeringdb-tables/${PEERINGDB_TABLE}.json&quot;
done
</code></pre>
<p>(see <a href="https://github.com/ii/org/blob/main/research/asn-data-pipeline/asn%5Fpipeline%5Fdocker%5Ffile.org#load-into-bigquery-dataset-and-prepare-vendor-data" 
  
   target="_blank" rel="noreferrer noopener" 
>stage load into BigQuery dataset and prepare vendor data</a>
)</p>
<p>The Potaroo ASN data is now joined with the PeeringDB data to add company name, website, and email.</p>
<pre><code class="language-sql-mode">copy (
  select distinct asn.asn,
  (net.data -&gt;&gt; 'name') as &quot;name&quot;,
  (net.data -&gt;&gt; 'website') as &quot;website&quot;,
  (poc.data -&gt;&gt; 'email') as email
  from asnproc asn
  left join peeriingdbnet net on (cast(net.data::jsonb -&gt;&gt; 'asn' as bigint) = asn.asn)
  left join peeriingdbpoc poc on ((poc.data -&gt;&gt; 'name') = (net.data -&gt;&gt; 'name'))
order by email asc) to '/tmp/peeringdb_metadata_prepare.csv' csv header;
</code></pre>
<p>It is then exported as CSV and uploaded to the BigQuery dataset.
(see <a href="https://github.com/ii/org/blob/main/research/asn-data-pipeline/asn%5Fpipeline%5Fdocker%5Ffile.org#load-and-combine-peeringdb--potaroo-asn-data" 
  
   target="_blank" rel="noreferrer noopener" 
>stage load and combind PeeringDB + Potaroo ASN data</a>
)</p>
<p>The Kubernetes Public Artifact Logs are then loaded into the BigQuery dataset in a table as the raw usage logs.
(see <a href="https://github.com/ii/org/blob/main/research/asn-data-pipeline/asn%5Fpipeline%5Fdocker%5Ffile.org#load-kubernetes-public-artifact-traffic-logs-into-bigquery-from-gcs-bucket" 
  
   target="_blank" rel="noreferrer noopener" 
>stage load Kubernetes Public Artifact Traffic Logs into BigQuery from GCS bucket</a>
)</p>
<p>Several queries are run against the BigQuery dataset to create some more handy tables, such as</p>
<ul>
<li>distinct IP count [from logs]; and</li>
<li>company name to ASN and IP block</li>
</ul>
<p>(see <a href="https://github.com/ii/org/blob/main/research/asn-data-pipeline/asn%5Fpipeline%5Fdocker%5Ffile.org#create-tables-in-bigquery-for-use-in-datastudio-dashboard" 
  
   target="_blank" rel="noreferrer noopener" 
>stage create tables in BigQuery for use in DataStudio dashboard</a>
)</p>
<p>With the heavy processing of the IPs done over in BigQuery, the data is pulled back through into Postgres with matches on IP to IP ranges. This is useful matching IP to IP range and then IP range to ASN.</p>
<pre><code class="language-sql-mode">copy
  (
    SELECT
      vendor_expanded_int.cidr_ip,
      vendor_expanded_int.start_ip,
      vendor_expanded_int.end_ip,
      vendor_expanded_int.asn,
      vendor_expanded_int.name_with_yaml_name,
      cust_ip.c_ip FROM vendor_expanded_int,
      cust_ip
    WHERE
      cust_ip.c_ip &gt;= vendor_expanded_int.start_ip_int
    AND
      cust_ip.c_ip &lt;= vendor_expanded_int.end_ip_int
  )
TO '/tmp/match-ip-to-iprange.csv' CSV HEADER;
</code></pre>
<p>The data is then pushed back to the BigQuery dataset, ready to be used.
(see <a href="https://github.com/ii/org/blob/main/research/asn-data-pipeline/asn%5Fpipeline%5Fdocker%5Ffile.org#prepare-local-data-of-ip-to-ip-range-in-postgres" 
  
   target="_blank" rel="noreferrer noopener" 
>stage prepare local data of IP to IP range in Postgres</a>
)</p>
<p>Addition tables in the BigQuery dataset are then made for joining</p>
<ul>
<li>usage data to IPs</li>
<li>company to IP</li>
</ul>
<p>with that all done, the dataset is complete.
(see <a href="https://github.com/ii/org/blob/main/research/asn-data-pipeline/asn%5Fpipeline%5Fdocker%5Ffile.org#connect-all-the-data-in-the-dataset-of-bigquery-together" 
  
   target="_blank" rel="noreferrer noopener" 
>stage connect all the data in the dataset of BigQuery together</a>
)</p>
<p>The last step of data processing is promoting the tables in the dataset to the <em>latest</em> / <em>stable</em> dataset which is picked up my DataStudio.
(see <a href="https://github.com/ii/org/blob/main/research/asn-data-pipeline/asn%5Fpipeline%5Fdocker%5Ffile.org#prepare-local-data-of-ip-to-ip-range-in-postgres" 
  
   target="_blank" rel="noreferrer noopener" 
>stage override the existing data used in the DataStudio report</a>
)</p>
<h2 id="datastudio-report"><a href="#datastudio-report" class="anchor">#</a>DataStudio report</h2>
<p>From the data produced, dashboards such as these can be made.</p>
<p>General stats</p>
<img style='margin-left: auto; margin-right: auto;' alt='k8s-infra-datastudio-repo-overview' src='/images/2021/k8s-infra-datastudio-repo-overview.png'>
<p>IP count and GB of traffic</p>
<img style='margin-left: auto; margin-right: auto;' alt='k8s-infra-datastudio-repo-ip-to-image' src='/images/2021/k8s-infra-datastudio-repo-ip-to-image.png'>
<p>(Please note: IPs and vendors will not be visible in report)</p>
<p>Download cost per-image</p>
<img style='margin-left: auto; margin-right: auto;' alt='k8s-infra-datastudio-repo-cost-per-image' src='/images/2021/k8s-infra-datastudio-repo-cost-per-image.png'>
<h2 id="moving-forward"><a href="#moving-forward" class="anchor">#</a>Moving forward</h2>
<p>The Kubernetes Infra Working Group (soon SIG) begun the effort to migrate infrastructure off Google owned GCP projects some time ago.
Through the use of this data, ii + CNCF + Kubernetes Infra WG will be able to reduce costs by assisting organisations who are pulling heavily from Kubernetes Artifact services migrate to their own hosted services.</p>
<p>An action item for the community is to reduce the size of artifacts, see <a href="https://github.com/kubernetes/kubernetes/issues/102493" 
  
   target="_blank" rel="noreferrer noopener" 
>https://github.com/kubernetes/kubernetes/issues/102493</a>
.</p>
<p>This container image is intended to be deployed into production as a Prow Job soon, so that the community can review its traffic cost per vendor, per IP, and per artifact.</p>
<h2 id="credits-and-special-thanks-to"><a href="#credits-and-special-thanks-to" class="anchor">#</a>Credits and special thanks to</h2>
<ul>
<li><a href="/author/berno-kleinhans" 
  
  
>Berno Kleinhans</a>
 for data discovery, manipulation, and pipeline building</li>
<li><a href="/author/riaan-kleinhans" 
  
  
>Riaan Kleinhans</a>
 for preparing the data and pulling it into a DataStudio report</li>
<li><a href="/author/zach-mandeville" 
  
  
>Zach Mandeville</a>
 for being incredible at building out database queries</li>
<li>Arnaud Meukam and Aaron Crickenberger both for time and energy to support implementing the infrastructure and access to it, advice on our changes and approaches, and for merging in what ii was in need of for doing this Prow Job</li>
</ul>


            
          </article>
          

 <div class="pagination">
  
    <a class="pagination__item" href="https://ii.nz/post/rerouting-container-registries-with-envoy/">
        <span class="pagination__label"></span>
        <span class="pagination__title">Rerouting Container Registries With Envoy</span>
    </a>
  

  
</div>

          
          <footer class="post__footer">
            


<div class="social-icons">
  
    
    
      <a class="social-icons__link" rel="me" title="GitLab"
         href="https://gitlab.com/ii"
         target="_blank" rel="noopener">
        <div class="social-icons__icon" style="background-image: url('https://ii.nz/svg/gitlab.svg')"></div>
      </a>
    
  
    
    
      <a class="social-icons__link" rel="me" title="GitHub"
         href="https://github.com/ii"
         target="_blank" rel="noopener">
        <div class="social-icons__icon" style="background-image: url('https://ii.nz/svg/github.svg')"></div>
      </a>
    
  
    
    
      <a class="social-icons__link" rel="me" title="Email"
         href="mailto:hello@ii.coop"
         target="_blank" rel="noopener">
        <div class="social-icons__icon" style="background-image: url('https://ii.nz/svg/email.svg')"></div>
      </a>
    
  
</div>

            <p>¬© 2023</p>
          </footer>
          </div>
      </div>
      
    </div>


  </main>

   

  
  <script src="/js/index.min.301a8b0870381bf76b3b5182e8966d363a0474281183439beb024d8b8228fc66.js" integrity="sha256-MBqLCHA4G/drO1GC6JZtNjoEdCgRg0Ob6wJNi4Io/GY=" crossorigin="anonymous"></script>
  
  
  <script src="https://unpkg.com/prismjs@1.20.0/components/prism-core.min.js"></script>

  
  <script src="https://unpkg.com/prismjs@1.20.0/plugins/autoloader/prism-autoloader.min.js"
    data-autoloader-path="https://unpkg.com/prismjs@1.20.0/components/"></script>

  


</body>

</html>
